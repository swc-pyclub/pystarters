{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using memmap\n",
    "Most of the times you will have to deal with data sets that are to big to fit in RAM. The solution to manipulating those is to use numpy.memmmap. That function gives you a pointer to memory in the hard disk where your data lives but is treated by numpy as a normal array.\n",
    "The thing with memmap arrays is that if you pass them to functions that do array manipulation then numpy will try to load the whole array in RAM. That will crash your computer. So manipulating such arrays needs to be done in parts (think loops). The trick here is to do this as efficiently as possible.\n",
    "\n",
    "## Random matrix and its covariance. Difficulty 2/5\n",
    "Let's start by setting up the problem. You will understand what the covariance function of numpy does by recreating it and runnign it on small arrays. This will give you an idea of how much slower the naive (loop based) approach is to the highly optimized numpy (vectorized) approach.\n",
    "\n",
    "1. Generate a random elements matrix with 1000 rows (the samples) and 100 columns (the dimensions).\n",
    "\n",
    "2. Calculate the covariance of this matrix using numpy.cov\n",
    "\n",
    "3. Time this\n",
    "\n",
    "4. Make some pairs of the matrix samples partially correlated<sup>*</sup> with each other. Create 200 such pairs (they should be randomly placed in the matrix but the relative position of the two samples in a pair can be either random or fixed). \n",
    "\n",
    "5. Report some covariances of the two matrices that show that on the pairs of samples you have chossen to increase the covariance that has happened.\n",
    "\n",
    "Notes:\n",
    "\n",
    "1. <sup>*</sup> To get perfect covariance then two samples would have to be identical (i.e. just copy one sample to the place of another). To get partial covariance one sample would have to be a little bit different to the other. That can be achieved by multiplying a copied sample's elements with random numbers that are close to one.\n",
    "\n",
    "2. Do not use loops for any of the above. Everything should be achieved with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "matrix_dimensions = (1000,100)\n",
    "correlated_pairs_amount = 200\n",
    "\n",
    "matrix = np.random.random(matrix_dimensions)\n",
    "\n",
    "t = time.clock()\n",
    "first_cov = np.cov(matrix)\n",
    "print('Time = ' + str(time.clock() - t) + 'sec')\n",
    "\n",
    "correlated_pairs_distances = np.random.randint(1, 100, correlated_pairs_amount)\n",
    "correlated_pairs_indices_first = np.random.randint(101, matrix_dimensions[0], correlated_pairs_amount)\n",
    "correlated_pairs_indices_second = correlated_pairs_indices_first - correlated_pairs_distances\n",
    "\n",
    "print('Average covariance of the pairs = ' + \\\n",
    "      str(np.mean(first_cov[correlated_pairs_indices_first, correlated_pairs_indices_second])))\n",
    "print('Covariance of a diagonal element = ' + str(first_cov[0,0]))\n",
    "\n",
    "correlation_additive = np.random.uniform(0.8, 0.9, (correlated_pairs_amount, matrix_dimensions[1]))\n",
    "\n",
    "matrix_cov = np.copy(matrix)\n",
    "matrix_cov[correlated_pairs_indices_first] = np.copy(matrix[correlated_pairs_indices_second]) * correlation_additive\n",
    "second_cov = np.cov(matrix_cov)\n",
    "\n",
    "print('Average covariance of the pairs = ' + \\\n",
    "      str(np.mean(second_cov[correlated_pairs_indices_first, correlated_pairs_indices_second])))\n",
    "print('Covariance of a diagonal element = ' + str(second_cov[0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the covariance of the same matrices using a multiple loop. Difficulty 3/5\n",
    "\n",
    "Now try and calulate the covariance by the most naive, loop based way possible.\n",
    "\n",
    "Have a look [here](http://stattrek.com/matrix-algebra/covariance-matrix.aspx) for a quick review of covariance\n",
    "\n",
    "Do things one element at a time and use as little numpy as possible (use it to calculate the row means)\n",
    "\n",
    "Check that the results of np.cov and your results are the same (print the mean of the square difs)\n",
    "\n",
    "Time how long that takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(matrix1, matrix2):\n",
    "    result = np.empty((matrix1.shape[0], matrix2.shape[0]))\n",
    "    \n",
    "    for i in range(matrix1.shape[0]):\n",
    "        mean1 = np.mean(matrix1[i])\n",
    "        for j in range(matrix2.shape[0]):\n",
    "            mean2 = np.mean(matrix2[j])\n",
    "            temp_sum = 0\n",
    "            for k in range(matrix1.shape[1]):\n",
    "                temp_sum += (matrix1[i, k] - mean1) * (matrix2[j, k] - mean2)\n",
    "            result[i, j] = temp_sum / matrix1.shape[1]\n",
    "    \n",
    "    return result\n",
    "\n",
    "t = time.clock()\n",
    "loop_first_cov = covariance(matrix, matrix)\n",
    "print('Time = ' + str(time.clock() - t) + 'sec')\n",
    "    \n",
    "print('First_Diff^2 = ' + str(np.mean(np.power(first_cov - loop_first_cov, 2))))\n",
    "\n",
    "t = time.clock()\n",
    "loop_second_cov = covariance(matrix_cov, matrix_cov)\n",
    "print('Time = ' + str(time.clock() - t) + 'sec')\n",
    "    \n",
    "print('Diff^2 = ' + str(np.mean(np.power(second_cov - loop_second_cov, 2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a large matrix (not fitting into RAM). Difficulty 1/5\n",
    "\n",
    "Use numpy.memmap to create a matrix whose elements are kept in the hard disk and not in RAM and that is so large that keeping it into RAM would be impossible.\n",
    "Make the matrix 10<sup>5</sup> x 10<sup>5</sup> elements. \n",
    "\n",
    "Fill this with random numbers.\n",
    "\n",
    "How much memory will that take? What else do you need to know to calculate the memory it will occupy other than its size?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path as path\n",
    "folder = r'E:\\Data\\temp_for_pystarters'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_file = path.join(folder, 'big_matrix.dat')\n",
    "matrix_dimensions = (10**5, 10**5)\n",
    "\n",
    "big_matrix_mm = np.memmap(matrix_file, np.float32, mode='w+', shape=matrix_dimensions)\n",
    "big_matrix_mm[:] = np.random.random(matrix_dimension)\n",
    "for i in range(big_matrix_mm.shape[0]):\n",
    "    big_matrix_mm[i, :] = np.random.random(big_matrix_mm.shape[1])\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the covariance of this matrix. Difficulty 5/5 (this is hard, do your best)\n",
    "Calulate the covariance of this matrix using\n",
    "1. The np.cov straight on the memmaped matrix. What happens?\n",
    "2. An as fast as possible loop (mix numpy vectorization and loops as you see fit to get the fastest possible result). To be as efficient as possible you might need to know how much RAM your system has so as to load as much as possible into memory thus minimizing your number of loop itterations. Use the psutil.virtual_memory() function (you might have to install the psutil library).\n",
    "\n",
    "Time how long your covariance function takes.\n",
    "\n",
    "Note:\n",
    "\n",
    "In order to develop your new covariance function that opperates on memmaped arrays use first some small memmaped matrices that you can do fast itterations of debuging on. While you are developing make sure that the covariance of any small matrix as calculated by np.cov is the same as the one you calculate. Also time both the np.cov function and your function on different size matrices to get a feel of how much worse you are doing and how that difference increases with matrix size. Once you are sure that everything works then run your function on the very large matrix.\n",
    "\n",
    "Make sure your function has print statements that allow you to follow its progress. It is very usefull to know if your function is running at all when it is expected to take a very long time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "debug_matrix_file = path.join(folder, 'debug_matrix.dat')\n",
    "debug_matrix_dimensions = (10**4, 10**4)\n",
    "\n",
    "debug_matrix_mm = np.memmap(debug_matrix_file, np.float32, mode='w+', shape=debug_matrix_dimensions)\n",
    "\n",
    "for i in range(debug_matrix_mm.shape[0]):\n",
    "    debug_matrix_mm[i, :] = np.random.random(debug_matrix_mm.shape[1])\n",
    "print('Finished making matrix')\n",
    "\n",
    "t = time.clock()\n",
    "np_cov = np.cov(debug_matrix_mm)\n",
    "print('Full numpy covariance took ' + str(time.clock() - t) + ' secs')\n",
    "\n",
    "\n",
    "def calculate_number_of_elements_fitting_in_ram():\n",
    "    remaining_memory = psutil.virtual_memory()[1]\n",
    "    mem_to_use = 0.7 * remaining_memory\n",
    "\n",
    "    total_elements_to_load_to_ram = int(mem_to_use / np.dtype(np.float32).itemsize)\n",
    "\n",
    "    # Use this to simulate smaller ram than what is in the computer for debugging\n",
    "    total_elements_to_load_to_ram = 1000\n",
    "\n",
    "    elements_of_matrix_to_ram = int(total_elements_to_load_to_ram / 2)\n",
    "\n",
    "    return elements_of_matrix_to_ram\n",
    "\n",
    "\n",
    "def get_number_of_iterations(matrix, num_of_elements):\n",
    "    total_rows = matrix.shape[0]\n",
    "    return int(np.ceil(total_rows / num_of_elements))\n",
    "\n",
    "\n",
    "def load_part_of_memmaped_matrix(matrix, num_of_elements_to_load, iteration_number):\n",
    "    starting_index = iteration_number * num_of_elements_to_load\n",
    "    ending_index = (iteration_number + 1) * num_of_elements_to_load\n",
    "    if ending_index > matrix.shape[0]:\n",
    "        ending_index = matrix.shape[0]\n",
    "\n",
    "    return matrix[starting_index:ending_index, :]\n",
    "\n",
    "\n",
    "def fill_in_covariance_mm(covariance, part_matrix1, part_matrix2, it1, it2, elements1, elements2):\n",
    "    start_index1 = elements1 * it1\n",
    "    end_index1 = elements1 * (it1 + 1)\n",
    "    if end_index1 > covariance.shape[0]:\n",
    "        end_index1 = covariance.shape[0]\n",
    "    start_index2 = elements2 * it2\n",
    "    end_index2 = elements2 * (it2 + 1)\n",
    "    if end_index2 > covariance.shape[0]:\n",
    "        end_index2 = covariance.shape[0]\n",
    "\n",
    "    full_covariance = np.cov(part_matrix1, part_matrix2)\n",
    "    cov_shape = full_covariance.shape\n",
    "    \n",
    "    # Putting results in the right place\n",
    "    covariance[start_index1:end_index1, start_index2:end_index2] = full_covariance[int(cov_shape[0]/2):int(cov_shape[0])\n",
    "                                                                                   , 0:int(cov_shape[1]/2)]\n",
    "    covariance[start_index2:end_index2, start_index1:end_index1] = full_covariance[0:int(cov_shape[0]/2),\n",
    "                                                                                   int(cov_shape[1]/2):int(cov_shape[1])]\n",
    "\n",
    "\n",
    "def covariance_mm(matrix1, matrix2, result_file):\n",
    "\n",
    "    covariance = np.memmap(result_file, np.float32, mode='w+', shape=(matrix1.shape[0], matrix2.shape[0]))\n",
    "\n",
    "    elements_of_matrix_to_ram = calculate_number_of_elements_fitting_in_ram()\n",
    "\n",
    "    for it1 in range(get_number_of_iterations(matrix1, elements_of_matrix_to_ram)):\n",
    "        for it2 in range(get_number_of_iterations(matrix2, elements_of_matrix_to_ram)):\n",
    "            part_matrix1 = load_part_of_memmaped_matrix(matrix1, elements_of_matrix_to_ram, it1)\n",
    "            part_matrix2 = load_part_of_memmaped_matrix(matrix2, elements_of_matrix_to_ram, it2)\n",
    "            fill_in_covariance_mm(covariance, part_matrix1, part_matrix2, it1, it2, elements_of_matrix_to_ram,\n",
    "                                  elements_of_matrix_to_ram)\n",
    "            \n",
    "            # print(it1, it2)\n",
    "\n",
    "    return covariance\n",
    "\n",
    "t = time.clock()\n",
    "result = covariance_mm(debug_matrix_mm, debug_matrix_mm, path.join(folder, 'debug.dat'))\n",
    "print('Partial numpy covariance took ' + str(time.clock() - t) + ' secs')\n",
    "\n",
    "\n",
    "dif = np.mean(np.power(result - np_cov,2))\n",
    "print(dif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief intro to Pandas\n",
    "Pandas is a package that makes it very easy to manipulate data sets with different labels. It's main data structures (DataFrame) are closer to excel spreadsheets and data bases than to numpy arrays. That allows you to book keep your complex data sets in intuitive ways and more importanlty to query your data (eg. Give me all the voltage timeseries for the trials where the animal was running faster than 3m/s).\n",
    "\n",
    "Pandas is extremely powerfull and this is a very basic introduction to it. For a  little bit more have a look [here](https://pandas.pydata.org/pandas-docs/version/0.22.0/10min.html).\n",
    "\n",
    "## Load some data into pandas DataFrames. Difficulty 2/5\n",
    "Load the Events.csv and the Video.csv into Pandas DataFrames (create two seperate DataFrames).\n",
    "The events file has the x, y position of a ball moving on a table, bouncing around walls. The time stamps of this file are the times that the calculation of the position of the ball happened. The video csv has the frame number of the camera recording the ball. The timestamps are the times that each frame was saved.\n",
    "\n",
    "Use the sep parameter of the pandas.read_csv funtion to import only the date/time, the name, the x position and the y position for the events and only the date/time and the frame number for the video file. Use the read_csv function to also get pandas to transform the date/time strings into Timestamp.\n",
    "\n",
    "The resulting DataFrames should have appropriately named columns and the data in each column should be of the correct type (Timestamp for the date/times, string for the name of the event, numbers (floats and ints) for the positions and the frame numbers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the two DataFrames into a single one. Difficulty 3/5\n",
    "Create a 3rd DataFrame that is the logical merge of the two DataFrames above. Keep only the 'common' rows. Use the rows that are closer in time between the events and the video frames.\n",
    "\n",
    "If you turn the date/time into indices then turn them back to normal column.\n",
    "\n",
    "The final DataFrame should have 5 columns, Time, Frame, Type, X, Y (with those or any other names you find appropriate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the positions of the ball. Difficult 1/5\n",
    "Plot all the 2d positions of the ball so that you can see its trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create direction columns. Difficulty 2/5\n",
    "Create a new column ('Direction') that records the angle (in degrees) of the movement of the ball in each frame.\n",
    "\n",
    "To check your result redo the previous plot but now colour code each position with its direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataFrame that has only the info of frames where there is a direction change. Difficulty 1/5\n",
    "To check the result plot these points on top of the previous scatter plot (but bigger and red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
